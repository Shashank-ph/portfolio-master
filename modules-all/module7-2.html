<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- 
    - primary meta tags
  -->
  <title>Shashank | Module 1</title>
  <meta name="title" content="Shashank | Services">
  <meta name="description" content="This is a personal portfolio">

  <!-- 
    - favicon
  -->
  <link rel="shortcut icon" href="../favicon.svg" type="image/svg+xml">

  <!-- 
    - google font link
  -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Jost:wght@400;500;600&display=swap" rel="stylesheet">

  <!-- 
    - custom css link
  -->
  <link rel="stylesheet" href="../assets/css/require.css">
  <link rel="stylesheet" href="../assets/css/pages/service.css">

  <style>
    #more {
      display: none;
    }

    #more1 {
      display: none;
    }

    #more2 {
      display: none;
    }
  </style>
</head>

<body>

  <!-- 
    - #PRELOADER
  -->

  <div class="preloader" data-preloader>
    <div class="preloader-circle"></div>
  </div>





  <!-- 
    - #HEADER
  -->

  <header class="header" data-header>
    <div class="container">

      <a href="../index.html" class="logo">
        <img src="../assets/images/SP-logo.png" width="200" height="50"
          alt="https://www.linkedin.com/in/shashank-phatak-26122b145/">
      </a>

      <nav class="navbar" data-navbar>
        <ul class="navbar-list">

          <li class="navbar-item">
            <a href="#" class="label-lg navbar-link has-after active">Home</a>
          </li>

          <li class="navbar-item">
            <a href="../service.html" class="label-lg navbar-link has-after">Services</a>
          </li>

          <li class="navbar-item">
            <a href="../modules.html" class="label-lg navbar-link has-after">Modules</a>
          </li>

          <li class="navbar-item">
            <a href="../portfolio.html" class="label-lg navbar-link has-after">Certificates</a>
          </li>

          <li class="navbar-item">
            <a href="../contact.html" class="label-lg navbar-link has-after">Contact</a>
          </li>

        </ul>
      </nav>

      <a href="../contact.html" class="btn btn-primary">Contact Now</a>

      <button class="nav-toggle-btn" aria-label="menu" data-nav-toggler>
        <ion-icon name="menu-outline" aria-hidden="true"></ion-icon>
      </button>

    </div>
  </header>






  <main>
    <article>

      <!-- 
        - #SERVICE
      -->

      <section class="section service has-bg-image" aria-labelledby="service-label"
        style="background-image: url('../assets/images/service-bg.png')">
        <div class="container">

          <h2 class="section-title headline-md text-center" id="service-label">Reflection on the ethics of generative AI
            Governance
          </h2>

          <ul class="service-list">

            <li class="card-container">
              <div class="card card-md" style="background-color: hsl(178, 30%, 69%)">

                <div>

                  <p class="body-sm">

                    The generative AI revolution, accelerating since late 2018, has transformed industries, societies,
                    and ethical landscapes, with computer science at its epicentre (Sengar et al., 2024). As Gu (2023)
                    argues, establishing a global consensus on AI governance values is critical yet challenging due to
                    diverse stakeholder perspectives and abstract normative discourse. This reflection evaluates the
                    global governance of generative AI, drawing on Gu (2023), Banh and Strobel (2023), and related
                    literature, to propose a multi-tiered framework addressing legal, social, and professional issues.
                    It incorporates specific governance documents and case studies to justify a balanced approach
                    fostering innovation and accountability.

                    <br><br>
                    <span id="dots"></span>
                    <span id="more">

                      <b>Understanding the Global AI Governance Landscape</b><br>
                      Gu (2023) emphasises the need for tools to catalogue and compare AI governance documents to
                      identify convergences and divergences across jurisdictions. Their analysis highlights varied
                      approaches, the US promotes innovation through the Executive Order on Safe, Secure, and
                      Trustworthy AI (White House, 2023). The EU prioritises human rights via the Artificial
                      Intelligence Act (European Parliament, 2023), and China focuses on state control with its 2019
                      Governance Principles for a New Generation of AI (Webster et al., 2017). These differences reflect
                      cultural, political, and economic priorities, complicating global consensus.<br><br>

                      Qian, Y. et al., (2024) explore generative AI’s societal impacts, particularly biases in large
                      language models, echoing Bender et al., (2021) critics of models like GPT argue that they
                      perpetuate biases and incur environmental costs. Additional literature, such as Murikah, W. et
                      al., (2024), highlights risks like misinformation, deepfakes, and privacy violations,
                      necessitating governance that balances innovation with ethical oversight. In Africa, Rwanda’s
                      National AI Policy (2022) emphasises local capacity building to counter data colonialism, where
                      Global North datasets dominate AI development, marginalising local contexts.<br><br>

                      <b>Legal, Social, and Professional Implications</b><br>
                      <b>Legal Implications:</b> Generative AI poses significant legal challenges. A 2023 US case saw
                      lawyers sanctioned for submitting AI-generated court filings with fabricated citations,
                      underscoring risks
                      of “hallucinations” (Bohannon, 2023). The EU’s AI Act mitigates such risks by mandating
                      transparency and risk assessments for high-risk AI systems (European Parliament, 2023). However,
                      its stringent regulations may deter innovation, particularly for SMEs. China’s data localisation
                      laws, under the Cybersecurity Law (2017), prioritise state control but limit cross-border
                      collaboration, creating legal fragmentation (Cheng and Zeng, 2023).<br><br>

                      <b>Social Implications:</b> Generative AI can erode trust through deepfakes and misinformation,
                      threatening democratic processes. A 2023 deepfake campaign in India manipulated political
                      narratives, highlighting societal risks (Pawelec, 2022). In Africa, biased datasets exacerbate
                      inequalities, as algorithms trained on Global North data fail to represent local cultures,
                      reinforcing digital divides (Pasipamire and Muroyiwa, 2024). Community engagement, as in Rwanda’s
                      AI policy, is crucial for equitable AI deployment.<br><br>

                      <b>Professional Implications:</b> Computer scientists face ethical dilemmas balancing innovation
                      with responsibility. The 2023 IEEE Ethically Aligned Design framework urges developers to
                      prioritise
                      fairness and transparency (White Paper - Ethically Aligned Design, 2019). Rapid industry demands
                      can often pressure professionals to deploy untested systems, risking harm and leading to biased
                      facial recognition technologies.<br><br>

                      <b>Proposed Course of Action</b><br>
                      To address these challenges, I propose a three-tiered governance framework: a global AI forum,
                      adaptive national policies, and industry-led ethical standards. This approach aligns with Gu
                      (2023) call for comparative tools and UNESCO’s (2021) multi-stakeholder principles.<br><br>

                      <b>1. Global AI Forum:</b> An UN-backed platform, modelled on the Intergovernmental Panel on
                      Climate Change, would facilitate dialogue among governments, industry, academia, and civil
                      society. It
                      would develop non-binding principles, building on UNESCO’s recommendation on the Ethics of AI
                      (2021), emphasising human rights, transparency, and fairness. The forum would create a digital
                      repository of governance documents to map shared values such as transparency in the EU’s AI Act
                      and US Executive Order and divergences (e.g., China’s state-centric approach). Rwanda’s National
                      AI Policy (2022) could inform inclusive strategies for developing nations.<br><br>

                      <b>2. Adaptive National Policies:</b> Countries should align with global principles while
                      tailoring regulations to local contexts. The US could enhance its 2023 Executive Order with
                      mandatory
                      third-party audits, ensuring accountability without rigorous innovation. The EU’s AI Act should
                      incorporate flexibility for SMEs, balancing regulation with competitiveness (European Parliament,
                      2023). In Africa, Rwanda’s focus on data sovereignty and capacity building could inspire policies
                      that reduce reliance on external datasets, fostering local innovation (Ministry of ICT, Rwanda,
                      2022).<br><br>

                      <b>3. Industry-Led Ethical Standards:</b> Tech companies should adopt voluntary codes,
                      prioritising explainability and bias mitigation. Initiatives like UNESCO’s Women4Ethical AI
                      platform, as
                      described by O’Hagan (2021) can ensure diverse representation in AI development, addressing
                      biases. Regular algorithmic audits, as Murikah, W. et al., (2024), would enhance transparency
                      while protecting proprietary interests through confidentiality agreements.<br><br>

                      <b>Impact of Proposed Actions</b><br>
                      Legal Impact: The global forum would harmonise standards, reducing conflicts between international
                      regulations. Adaptive national policies would ensure compliance while fostering innovation.
                      Industry audits would provide legal accountability, aligning with the EU’s risk-based approach
                      (European Parliament, 2023).<br><br>

                      <b>Social Impact:</b> Inclusive governance would address biases, particularly in underrepresented
                      regions. Rwanda’s community-driven AI policy exemplifies how local engagement can promote equity
                      (Ministry of ICT, Rwanda, 2022). Transparent systems would reduce misinformation risks, as seen in
                      India’s deepfake incidents, fostering societal trust.<br>

                      <b>Professional Impact:</b> Clear ethical guidelines would empower computer scientists to navigate
                      dilemmas, supported by IEEE’s frameworks (2023). Training programs, like those in Rwanda’s AI
                      ecosystem, would equip professionals with skills to develop responsible AI (Ministry of ICT,
                      Rwanda, 2022).<br><br>


                      <b>Justification and Challenges</b><br>
                      This framework builds on Gu (2023) tools for governance comparison and UNESCO’s (2021) ethical
                      principles. It addresses Murikah, W. et al., (2024) concerns about bias and environmental costs
                      through transparency and local data use. Case studies, like Rwanda’s policy and the EU’s AI Act,
                      demonstrate practical applications. However, geopolitical tensions, as Cheng and Zeng (2023) notes
                      may hinder consensus. Industry resistance to audits and high implementation costs in developing
                      nations are challenges. These can be mitigated through phased adoption, financial support via
                      international aid, and incentives like tax benefits for ethical AI practices.<br><br>

                      <b>Conclusion</b><br>
                      Generative AI’s transformative potential requires governance that balances innovation with ethical
                      responsibility. A global forum, adaptive national policies, and industry standards offer a path
                      toward consensus while respecting diversity. By addressing legal accountability, social equity,
                      and professional ethics, this framework ensures AI serves humanity responsibly, fostering trust
                      and sustainable progress in computer science and beyond.<br><br>

                      <b>References</b><br>
                      Banh, L. and Strobel, G. (2023) ‘Generative artificial intelligence’, Electronic Markets, 33(1),
                      p. 63. Available at: https://doi.org/10.1007/s12525-023-00680-1.<br><br>

                      Bender, E.M. et al. (2021) ‘On the Dangers of Stochastic Parrots: Can Language Models Be Too
                      Big?’, in Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency.
                      FAccT ’21: 2021 ACM Conference on Fairness, Accountability, and Transparency, Virtual Event
                      Canada: ACM, pp. 610–623. Available at: https://doi.org/10.1145/3442188.3445922.<br><br>

                      Bohannan, M. (2023) Lawyer Used ChatGPT In Court—And Cited Fake Cases. A Judge Is Considering
                      Sanctions. Forbes. Available from:
                      https://www.forbes.com/sites/mollybohannon/2023/06/08/lawyer-used-chatgpt-in-court-and-cited-fake-cases-a-judge-is-considering-sanctions/
                      [Accessed 7th August 2025]<br><br>

                      Cheng, J. and Zeng, J. (2023) ‘Shaping AI’s Future? China in Global AI Governance’, Journal of
                      Contemporary China, 32(143), pp. 794–810. Available at:
                      https://doi.org/10.1080/10670564.2022.2107391.<br><br>

                      European Parliament. (2023) EU AI Act: first regulation on artificial intelligence. Available
                      from:
                      https://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence
                      [Accessed 6th August 2025]<br><br>

                      Gu, P. (2023) ‘Global Artificial Intelligence Governance: Challenges and Complications’, Science
                      Insights, 42(6), pp. 975–979. Available at: https://doi.org/10.15354/si.23.re596.<br><br>

                      Ministry of ICT and Innovation, Republic of Rwanda. (2022) The National AI Policy. Available from:
                      https://rura.rw/fileadmin/Documents/ICT/Laws/Rwanda_national_Artificial_intelligence_Policy.pdf
                      [Accessed 7th August 2025]<br><br>

                      Murikah, W. et al., (2024) ‘Bias and ethics of AI systems applied in auditing - A systematic
                      review’, Scientific African, 25, p. e02281. Available at:
                      https://doi.org/10.1016/j.sciaf.2024.e02281.<br><br>

                      O'Hagan, C. (2023) Artificial Intelligence: UNESCO launches Women4Ethical AI expert platform to
                      advance gender equality. Available from:
                      https://www.unesco.org/en/articles/artificial-intelligence-unesco-launches-women4ethical-ai-expert-platform-advance-gender-equality
                      [Accessed 10th August 2025]<br><br>

                      Pasipamire, N. and Muroyiwa, A. (2024) ‘Navigating algorithm bias in AI: ensuring fairness and
                      trust in Africa’, Frontiers in Research Metrics and Analytics, 9, p. 1486600. Available at:
                      https://doi.org/10.3389/frma.2024.1486600.<br><br>

                      Pawelec, M. (2022) ‘Deepfakes and Democracy (Theory): How Synthetic Audio-Visual Media for
                      Disinformation and Hate Speech Threaten Core Democratic Functions’, Digital Society, 1(2), p. 19.
                      Available at: https://doi.org/10.1007/s44206-022-00010-6.<br><br>

                      Qian, Y. et al. (2024) ‘Societal impacts of artificial intelligence: Ethical, legal, and
                      governance issues’, Societal Impacts, 3, p. 100040. Available at:
                      https://doi.org/10.1016/j.socimp.2024.100040.<br><br>

                      Sengar, S.S. et al. (2024) ‘Generative artificial intelligence: a systematic review and
                      applications’, Multimedia Tools and Applications, 84(21), pp. 23661–23700. Available at:
                      https://doi.org/10.1007/s11042-024-20016-1.<br><br>

                      The White House (2023). Executive Order on the Safe, Secure, and Trustworthy Development and Use
                      of Artificial Intelligence Available from:
                      https://bidenwhitehouse.archives.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/
                      [Accessed 10th August 2025]<br><br>

                      UNESCO (2021). Recommendation on the Ethics of AI. Available from:
                      https://ethicstech.org/unescothe-ethics-of-artificial-intelligence-2021/ [Accessed 10th August
                      2025]<br><br>

                      Webster, G. et al., (2017) Full Translation: China's 'New Generation Artificial Intelligence
                      Development Plan' (2017). Available from:
                      https://www.newamerica.org/cybersecurity-initiative/digichina/blog/full-translation-chinas-new-generation-artificial-intelligence-development-plan-2017/
                      [Accessed 10th August 2025]<br><br>

                      White Paper - Ethically Aligned Design - A Vision for Prioritizing Human Well-being with
                      Autonomous and Intelligent Systems (2019). S.l.: IEEE.<br><br>


                  </p>
                  </span>
                  <button onclick="myFunction()" id="myBtn"><b>Read more</b></button>


            </li>

          </ul>

        </div>
      </section>

    </article>
  </main>





  <!-- 
    - #FOOTER
  -->

  <footer class="footer">
    <div class="container">

      <ul class="social-list">

        <li>
          <a href="#" class="social-link">
            <ion-icon name="logo-facebook"></ion-icon>
          </a>
        </li>

        <li>
          <a href="#" class="social-link">
            <ion-icon name="logo-twitter"></ion-icon>
          </a>
        </li>

        <li>
          <a href="#" class="social-link">
            <ion-icon name="logo-instagram"></ion-icon>
          </a>
        </li>

        <li>
          <a href="#" class="social-link">
            <ion-icon name="logo-linkedin"></ion-icon>
          </a>
        </li>

        <li>
          <a href="#" class="social-link">
            <ion-icon name="logo-pinterest"></ion-icon>
          </a>
        </li>

      </ul>

      <p class="text-center">&copy; 2025 copyright all right reserved</p>

    </div>
  </footer>





  <!-- 
    - custom js link
  -->
  <script src="../assets/js/script.js"></script>

  <!-- 
    - ionicon
  -->
  <script type="module" src="https://unpkg.com/ionicons@5.5.2/dist/ionicons/ionicons.esm.js"></script>
  <script nomodule src="https://unpkg.com/ionicons@5.5.2/dist/ionicons/ionicons.js"></script>


  <script>
    function myFunction() {
      var dots = document.getElementById("dots");
      var moreText = document.getElementById("more");
      var btnText = document.getElementById("myBtn");

      if (dots.style.display === "none") {
        dots.style.display = "inline";
        btnText.innerHTML = "Read more";
        moreText.style.display = "none";
      } else {
        dots.style.display = "none";
        btnText.innerHTML = "Read less";
        moreText.style.display = "inline";
      }
    }

    function myFunction1() {
      var dots1 = document.getElementById("dots1");
      var moreText1 = document.getElementById("more1");
      var btnText1 = document.getElementById("myBtn1");

      if (dots1.style.display === "none") {
        dots1.style.display = "inline";
        btnText1.innerHTML = "Read more";
        moreText1.style.display = "none";
      } else {
        dots1.style.display = "none";
        btnText1.innerHTML = "Read less";
        moreText1.style.display = "inline";
      }
    }

    function myFunction2() {
      var dots2 = document.getElementById("dots2");
      var moreText2 = document.getElementById("more2");
      var btnText2 = document.getElementById("myBtn2");

      if (dots2.style.display === "none") {
        dots2.style.display = "inline";
        btnText2.innerHTML = "Read more";
        moreText2.style.display = "none";
      } else {
        dots2.style.display = "none";
        btnText2.innerHTML = "Read less";
        moreText2.style.display = "inline";
      }
    }
  </script>
</body>

</html>