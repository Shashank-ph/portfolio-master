<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- 
    - primary meta tags
  -->
  <title>Shashank | Module 1</title>
  <meta name="title" content="Shashank | Services">
  <meta name="description" content="This is a personal portfolio">

  <!-- 
    - favicon
  -->
  <link rel="shortcut icon" href="../favicon.svg" type="image/svg+xml">

  <!-- 
    - google font link
  -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Jost:wght@400;500;600&display=swap" rel="stylesheet">

  <!-- 
    - custom css link
  -->
  <link rel="stylesheet" href="../assets/css/require.css">
  <link rel="stylesheet" href="../assets/css/pages/service.css">

  <style>
    #more {
      display: none;
    }

    #more1 {
      display: none;
    }

    #more2 {
      display: none;
    }
  </style>
</head>

<body>

  <!-- 
    - #PRELOADER
  -->

  <div class="preloader" data-preloader>
    <div class="preloader-circle"></div>
  </div>





  <!-- 
    - #HEADER
  -->

  <header class="header" data-header>
    <div class="container">

      <a href="../index.html" class="logo">
        <img src="../assets/images/SP-logo.png" width="200" height="50"
          alt="https://www.linkedin.com/in/shashank-phatak-26122b145/">
      </a>

      <nav class="navbar" data-navbar>
        <ul class="navbar-list">

          <li class="navbar-item">
            <a href="#" class="label-lg navbar-link has-after active">Home</a>
          </li>

          <li class="navbar-item">
            <a href="../service.html" class="label-lg navbar-link has-after">Services</a>
          </li>

          <li class="navbar-item">
            <a href="../modules.html" class="label-lg navbar-link has-after">Modules</a>
          </li>

          <li class="navbar-item">
            <a href="../portfolio.html" class="label-lg navbar-link has-after">Certificates</a>
          </li>

          <li class="navbar-item">
            <a href="../contact.html" class="label-lg navbar-link has-after">Contact</a>
          </li>

        </ul>
      </nav>

      <a href="../contact.html" class="btn btn-primary">Contact Now</a>

      <button class="nav-toggle-btn" aria-label="menu" data-nav-toggler>
        <ion-icon name="menu-outline" aria-hidden="true"></ion-icon>
      </button>

    </div>
  </header>






  <main>
    <article>

      <!-- 
        - #SERVICE
      -->

      <section class="section service has-bg-image" aria-labelledby="service-label"
        style="background-image: url('../assets/images/service-bg.png')">
        <div class="container">

          <h2 class="section-title headline-md text-center" id="service-label">Enhancing Cyberbullying Detection Through
            Hybrid Machine Learning Models on Social Media Platforms: A Multimodal and Ethical Approach
          </h2>

          <ul class="service-list">

            <li class="card-container">
              <div class="card card-md" style="background-color: hsl(178, 30%, 69%)">

                <div>

                  <p class="body-sm">

                    <b>Significance / Research Problem</b><br>
                    Cyberbullying, defined as intentional and repeated online harassment (Olweus, 2012), poses a
                    significant public health and cybersecurity challenge, affecting up to 37% of adolescents and
                    leading to anxiety, depression, and suicidal ideation (Patchin and Hinduja, 2015). The research
                    problem lies in the limitations of current detection systems: manual moderation is inefficient for
                    the scale of social media data (e.g., billions of posts daily on platforms like X and Instagram),
                    and existing ML models often achieve only 70-85% accuracy, struggling with multimodal content (text,
                    images, videos), cultural nuances, and ethical biases (Rosa et al., 2019; Salawu et al., 2020). This
                    gap results in delayed interventions and perpetuated harm, particularly in diverse regions like
                    India, the US, and the UK, where linguistic and cultural variations exacerbate detection failures
                    (Al-Garadi et al., 2019).

                    <br><br>
                    <span id="dots"></span>
                    <span id="more">

                      The significance of this project is its contribution to the computing discipline by developing a
                      hybrid ML model that integrates deep learning (DL) and large language models (LLMs) to achieve
                      over 85% F1-scores, addressing multimodal data and biases (Devlin et al., 2019). It advances
                      cybersecurity under CyBOK's Human Factors by incorporating user feedback and ethical audits,
                      potentially reducing cyberbullying incidents by 15-20% through the use of real-time alerts (Kumari
                      et al., 2021). This fills a critical void in scalable, culturally sensitive tools, contributing to
                      policy frameworks for safer digital environments and interdisciplinary research in AI ethics
                      (Binns, 2017).<br><br>

                      <b>Research Question</b><br>
                      How can hybrid machine learning models, incorporating multimodal data and user feedback, improve
                      the accuracy, ethical fairness, and real-world applicability of cyberbullying detection on social
                      media platforms like X and Instagram?<br><br>

                      <b>Aims and Objectives</b><br>
                      <b>Aim:</b> To design, develop, and evaluate a hybrid ML system for accurate, ethical, and
                      proactive
                      cyberbullying detection, reducing harm on social media while addressing cultural and multimodal
                      challenges.<br><br>

                      <b>Objectives:</b><br>
                      1. To synthesise and critically evaluate key literature on ML-based cyberbullying detection from
                      2000-2025, identifying gaps in accuracy and ethics (Rosa et al., 2019; Salawu et al.,
                      2020).<br><br>
                      2. To develop a hybrid model combining CNN for image analysis, LSTM for text sequences, and
                      fine-tuned BERT for contextual detection, targeting 75%+ F1-scores (Kumari et al., 2021; Devlin et
                      al., 2019).<br><br>
                      3. To collect and preprocess diverse datasets from X and Instagram, incorporating cultural
                      variations from India, the US, and the UK, to mitigate biases (Al-Garadi et al., 2019).<br><br>
                      4. To integrate user feedback loops and ethical audits to enhance model transparency and fairness,
                      reducing false positives by 20% (Van Hee et al., 2018; Binns, 2017).<br><br>
                      5. To pilot the system in a real-world setting, evaluating its impact on cyberbullying reduction
                      through data analysis and user surveys (Patchin and Hinduja, 2015).<br><br>

                      <b>Key Literature Related to the Project</b><br>
                      The literature from 2000 to 2025 highlights ML's evolution in cyberbullying detection. Rosa et al.
                      (2019) provide a systematic review showing traditional ML (e.g., SVM) at 70-85% accuracy, limited
                      by feature engineering, while DL models like LSTM improve to 85-95% F1-scores by capturing context
                      (Badjatiya et al., 2017). Kumari et al. (2021) advanced multimodal detection with CNN-LSTM hybrids
                      for Instagram, achieving 90% precision but noting scalability issues. Devlin et al. (2019)
                      introduce BERT for contextual understanding, enabling greater accuracy in multilingual settings
                      (Al-Garadi et al., 2019). Ethical concerns, such as bias in diverse datasets, are critiqued by
                      Binns (2017), who advocates fairness audits. Patchin and Hinduja (2015) link detection to
                      psychological outcomes, emphasising user-centric interventions. Ptaszynski et al. (2018) address
                      cultural biases in non-English data, while Salawu et al. (2020) call for federated learning to
                      preserve privacy. Critically, these works reveal gaps in real-world deployment and
                      interdisciplinary integration, which this project aims to fill by proposing hybrid models with
                      explainable AI (Van Hee et al., 2018). Literature supports a mixed-methods approach, but it lacks
                      a focus on the longitudinal impact, a key area for this proposal.<br><br>

                      <b>Methodology / Development Strategy / Research Design</b><br>
                      This project employs a mixed-methods design, combining quantitative ML development with
                      qualitative evaluation, suitable for computing research (Creswell and Clark, 2017). The
                      development strategy follows an iterative agile process: Phase 1 involves data collection via web
                      scraping and APIs from X and Instagram, pre-processed with NLP tools like tokenisation and
                      oversampling to handle imbalance (Al-Garadi et al., 2019). Phase 2 builds the hybrid model using
                      for CNN-LSTM integration, and BERT fine-tuning, trained on a GPU cluster (Devlin et al., 2019;
                      Kumari et al., 2021). Model evaluation utilises cross-validation, F1-scores, and precision/recall
                      metrics, and is compared against benchmarks such as SVM (Rosa et al., 2019). Qualitative data from
                      user surveys will assess feedback loops (Patchin and Hinduja, 2015). A pilot deployment on a
                      simulated platform will test real-time performance. Data analysis processes include statistical
                      tests and bias audits (Binns, 2017). Critically, this design critiques over-reliance on
                      quantitative metrics, incorporating qualitative insights to achieve holistic validity, although
                      computational demands may limit its scalability.<br><br>

                      <b>Ethical Considerations and Risk Assessment</b><br>
                      Ethical approval will be sought through emphasising informed consent, anonymity, and data
                      minimisation, in compliance with the GDPR (Salawu et al., 2020). Participants in surveys will
                      receive debriefing and support resources to mitigate psychological risks from recalling
                      cyberbullying experiences (Patchin and Hinduja, 2015). Data will be anonymised and stored
                      securely, with opt-out options. Bias risks, such as cultural misrepresentation in datasets, will
                      be assessed using fairness metrics, reducing potential harm to underrepresented groups (Binns,
                      2017). A risk assessment identifies low-probability, high-impact issues, such as data breaches and
                      model false positives leading to censorship (Van Hee et al., 2018). Medium risks include
                      annotation subjectivity, which is countered by the involvement of diverse annotators. Overall, the
                      project prioritises beneficence and justice, critically evaluating how ethical lapses in
                      literature (e.g., unaddressed biases) can be avoided (Al-Garadi et al., 2019).<br><br>

                      <b>Timeline of Proposed Activities</b><br>
                      <b>Month 1-3:</b> Literature review update and data collection.<br>
                      <b>Month 4-6:</b> Model development and training.<br>
                      <b>Months 7-9:</b> Ethical audits and pilot testing.<br>
                      <b>Months 10-12:</b> User surveys, data analysis, and refinement.<br>
                      <b>Month 13-18:</b> Final evaluation and dissemination.<br>
                      <b>Month 19-24:</b> Scaling and reporting.<br>

                      This 2-year timeline includes buffers for iterations, critically addressing delays noted in
                      similar projects.<br><br>

                      <b>References</b><br>
                      Al-Garadi, M.A. et al. (2019) Predicting Cyberbullying on social media in the Big Data Era Using
                      Machine Learning Algorithms: Review of Literature and Open Challenges, IEEE Access, 7, pp.
                      70701–70718.<br><br>

                      Binns, R. (2017) Fairness in Machine Learning: Lessons from Political Philosophy. Available at:
                      https://doi.org/10.48550/ARXIV.1712.03586.<br><br>

                      Creswell, J.W. and Clark, V.L.P. (2017) Designing and conducting mixed methods research. 3rd edn.
                      Thousand Oaks, CA: Sage Publications.<br><br>

                      Devlin, J. et al. (2019) “BERT: Pre-training of Deep Bidirectional Transformers for Language
                      Understanding,” in Proceedings of the 2019 Conference of the North. Proceedings of the 2019
                      Conference of the North, Minneapolis, Minnesota: Association for Computational Linguistics, pp.
                      4171–4186.<br><br>

                      Kumari, K. et al. (2021) Multi-modal aggression identification using Convolutional Neural Network
                      and Binary Particle Swarm Optimization, Future Generation Computer Systems, 118, pp. 187–197.
                      Available at: https://doi.org/10.1016/j.future.2021.01.014.<br><br>

                      Olweus, D. (2012) 'Cyberbullying: An overrated phenomenon?', European Journal of Developmental
                      Psychology, 9(5), pp. 520-538.<br><br>

                      Patchin, J.W. and Hinduja, S. (2015) Measuring cyberbullying: Implications for research,
                      Aggression and Violent Behavior, 23, pp. 69–74. Available at:
                      https://doi.org/10.1016/j.avb.2015.05.013.<br><br>

                      Ptaszynski, M. et al. (2018) 'Brute force cyberbullying detection based on parent-child
                      relationship of comment threads', 2018 IEEE International Conference on Data Mining Workshops
                      (ICDMW), pp. 198-203.<br><br>

                      Rosa, H. et al. (2019) Automatic cyberbullying detection: A systematic review, Computers in Human
                      Behavior, 93, pp. 333–345. Available at: https://doi.org/10.1016/j.chb.2018.12.021.<br><br>

                      Salawu, S. et al., (2020) Approaches to Automated Detection of Cyberbullying: A Survey, IEEE
                      Transactions on Affective Computing, 11(1), pp. 3–24. Available at:
                      https://doi.org/10.1109/TAFFC.2017.2761757.<br><br>

                      Van Hee, C. et al. (2018) Automatic detection of cyberbullying in social media text, PLOS ONE.
                      Edited by H. Suleman, 13(10), p. e0203794. Available at:
                      https://doi.org/10.1371/journal.pone.0203794.
                      <br><br>
                  </p>
                  </span>
                  <button onclick="myFunction()" id="myBtn"><b>Read more</b></button>


            </li>

          </ul>

        </div>
      </section>

    </article>
  </main>





  <!-- 
    - #FOOTER
  -->

  <footer class="footer">
    <div class="container">

      <ul class="social-list">

        <li>
          <a href="#" class="social-link">
            <ion-icon name="logo-facebook"></ion-icon>
          </a>
        </li>

        <li>
          <a href="#" class="social-link">
            <ion-icon name="logo-twitter"></ion-icon>
          </a>
        </li>

        <li>
          <a href="#" class="social-link">
            <ion-icon name="logo-instagram"></ion-icon>
          </a>
        </li>

        <li>
          <a href="#" class="social-link">
            <ion-icon name="logo-linkedin"></ion-icon>
          </a>
        </li>

        <li>
          <a href="#" class="social-link">
            <ion-icon name="logo-pinterest"></ion-icon>
          </a>
        </li>

      </ul>

      <p class="text-center">&copy; 2025 copyright all right reserved</p>

    </div>
  </footer>





  <!-- 
    - custom js link
  -->
  <script src="../assets/js/script.js"></script>

  <!-- 
    - ionicon
  -->
  <script type="module" src="https://unpkg.com/ionicons@5.5.2/dist/ionicons/ionicons.esm.js"></script>
  <script nomodule src="https://unpkg.com/ionicons@5.5.2/dist/ionicons/ionicons.js"></script>


  <script>
    function myFunction() {
      var dots = document.getElementById("dots");
      var moreText = document.getElementById("more");
      var btnText = document.getElementById("myBtn");

      if (dots.style.display === "none") {
        dots.style.display = "inline";
        btnText.innerHTML = "Read more";
        moreText.style.display = "none";
      } else {
        dots.style.display = "none";
        btnText.innerHTML = "Read less";
        moreText.style.display = "inline";
      }
    }

    function myFunction1() {
      var dots1 = document.getElementById("dots1");
      var moreText1 = document.getElementById("more1");
      var btnText1 = document.getElementById("myBtn1");

      if (dots1.style.display === "none") {
        dots1.style.display = "inline";
        btnText1.innerHTML = "Read more";
        moreText1.style.display = "none";
      } else {
        dots1.style.display = "none";
        btnText1.innerHTML = "Read less";
        moreText1.style.display = "inline";
      }
    }

    function myFunction2() {
      var dots2 = document.getElementById("dots2");
      var moreText2 = document.getElementById("more2");
      var btnText2 = document.getElementById("myBtn2");

      if (dots2.style.display === "none") {
        dots2.style.display = "inline";
        btnText2.innerHTML = "Read more";
        moreText2.style.display = "none";
      } else {
        dots2.style.display = "none";
        btnText2.innerHTML = "Read less";
        moreText2.style.display = "inline";
      }
    }
  </script>
</body>

</html>